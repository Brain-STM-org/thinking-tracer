# AGENTS: `thinking-tracer`

## Project Overview

`thinking-tracer` is allows for the visualization of LLM conversations.  It allows a user to dissect a conversation (it's "trace"):  the prompts, the resultant thinking, the output, and the metadata.  This is intended for 3D environments, so we will use WebGL (or WebGPU) in a web page.  It is a user interface component that can be embedded in other applications.  We include a sample program that a user can run locally and drag-and-drop their conversations and view it.

We hope to support multiple agents, but we will start with Claude Code files.

Released under the MIT license, see [LICENSE.txt](./LICENSE.txt)

## Documentation Map

We maintain a series of Markdown documents to facilitate this project.

| File | Purpose | Editable |
|------|---------|----------|
| README.md | Human-readable project overview | Yes |
| AGENTS.md | Agent instructions (this file) | Yes |
| MEMORY.md | Long-term concepts and decisions | Yes |
| PLAN.md | Implementation planning | Yes |
| PROMPTS.md | Auto-generated by hook | **READ ONLY** |

### Research Reports

| File | Topic |
|------|-------|
| [etc/reports/3d_libraries.md](./etc/reports/3d_libraries.md) | 3D framework evaluation (Three.js, Babylon, R3F) |
| [etc/reports/clustering_ux_research.md](./etc/reports/clustering_ux_research.md) | Graph clustering and expand/collapse UX patterns |

## Research: Amp Code Trace/Thread Features

This section documents research on [Amp Code](https://ampcode.com/) (by Sourcegraph) and what users find most valuable about its conversation/thread features. This analysis informs design decisions for `thinking-tracer`.

### What Amp Threads Are

Amp organizes conversations as **threads**—"conversations with the agent, containing all your messages, context, and tool calls." Each thread maintains its own context, file changes, and conversation history, functioning like Git branches for AI conversations.

### Most Valued Features (User Feedback)

#### 1. Thread Persistence & Accessibility
Users consistently cite thread persistence as a **favorite feature**:
- Threads stored server-side allow resuming from any device
- "Start work on my laptop and finish it later on a server or phone"
- Threads serve as "living memory for your projects"
- Addresses the pain of lost context between sessions

#### 2. Sharing & Collaboration
Thread sharing enables powerful team workflows:
- Multiple visibility levels: public, unlisted, workspace-shared, private
- Teams include thread links in code reviews for reviewer context
- "Reading your team's threads helps you see what's going on"
- Learn from how colleagues solve problems
- Public profiles allow studying others' approaches

#### 3. Search & Discovery
- Search past threads by keyword, file path, repository, author, date, or task
- Reference other threads using URLs/IDs (`@T-7f395a45...`)
- Cross-reference to extract relevant information

#### 4. Context Management & Visualization
- Context usage indicators (warns at 80% capacity)
- Thread compaction (reduced usage from 80% to 6% in one test)
- Built-in to-do lists keep agent "on track"
- Sub-agents report results back to main thread

#### 5. Edit & Undo Capabilities
- Edit prior messages, automatically reverting changes made after that point
- Individual file changes can be reverted separately
- Provides control and safety when exploring approaches

### Design Implications for thinking-tracer

| Amp Feature | Design Insight |
|-------------|----------------|
| Thread persistence | Users value being able to return to and study past conversations |
| Sharing with visibility controls | Public/private sharing is important for both collaboration and privacy |
| Search by multiple dimensions | Index conversations by file, repo, date, keywords for discovery |
| Context visualization | Show context/token usage visually; users care about capacity |
| Tool call visibility | Display tool calls explicitly—"tool usage makes an agent come to life" |
| Edit/revert history | Visualize conversation branches and what-if exploration |
| Sub-agent structure | Show hierarchical relationships between main threads and sub-agents |

### Key Quotes from Users

> "My favorite part about @AmpCode is that you can share your whole session globally. PRs with Amp threads attached make me very, very happy as a maintainer..."

> "The thread messages (both input and output) can be accessed online and can even be shared with your team or publicly"

> "Thread sharing and leaderboards allows dev teams to collaborate and build together"

### Amp Research Conclusion

Amp focuses on **workflow-level** features (persistence, sharing, collaboration) rather than **structural navigation** of conversation internals. Users value these workflow features, but there's little emphasis on:
- Visualizing prompt → thinking → output relationships
- Navigating internal turn structure (tool calls, reasoning blocks)
- Understanding *why* the model made specific decisions

---

## Research: Reasoning Structure Visualization

This section covers academic research on visualizing LLM reasoning, which is more relevant to `thinking-tracer`'s goal of dissecting conversation structure.

### The Gap in Current Tools

| Focus Area | Examples | What It Covers |
|------------|----------|----------------|
| **Workflow** | Amp threads | Persistence, sharing, collaboration |
| **Neural interpretability** | Anthropic's "AI microscope" | Model internals, feature circuits |
| **Single-turn reasoning** | Hippo, Landscape of Thoughts | Chain-of-thought visualization |
| **Conversation structure** | **(Underserved)** | Multi-turn prompt/thinking/output navigation |

**Key insight**: Tools either operate at the workflow level (threads as units) or at the neural level (model internals). Few tools help users navigate the *structure within* a conversation—the relationships between prompts, thinking blocks, tool calls, and outputs across multiple turns.

### Hippo: Interactive Reasoning (UIST 2025)

[Hippo](https://arxiv.org/html/2506.23678v1) transforms reasoning chains into interactive tree visualizations. Key findings:

#### UI/UX Patterns
- **Tree over linear text**: Hierarchical display significantly improved comprehension vs. "wall of text"
- **Progressive disclosure**: Collapsible subtrees with summaries reduce cognitive load
- **Real-time streaming**: Users watch reasoning unfold token-by-token
- **Bi-directional linking**: Highlight connections between reasoning nodes and final response sentences

#### User Interactions
- **Edit**: Revise reasoning nodes inline
- **Branch**: Add child nodes with custom prompts to steer reasoning
- **Delete/Regenerate**: Remove or update reasoning paths
- **Collapse**: Hide subtrees while preserving summaries

#### Key User Insights
1. **Breadth-first preferred**: Users wanted to see conceptual structure first, then drill down—opposite of the depth-first generation order
2. **Reasoning > final answer**: For complex decisions, the thinking process was often more valuable than the output
3. **"Wall of text" problem**: Linear reasoning display created "significant cognitive barriers"
4. **Traceability valued**: Connecting reasoning nodes to output sentences helped users understand provenance

#### Design Implications
| Hippo Finding | Implication for thinking-tracer |
|---------------|--------------------------------------|
| Tree structure > linear | Use hierarchical/spatial layout for conversation structure |
| Breadth-first exploration | Show overview first, allow drilling into details |
| Reasoning as artifact | Treat thinking blocks as first-class viewable content |
| Traceability | Link outputs back to the prompts/thinking that produced them |
| Collapsible sections | Support progressive disclosure for long conversations |

### Anthropic: Tracing Thoughts in Language Models

[Anthropic's interpretability research](https://www.anthropic.com/research/tracing-thoughts-language-model) explores whether stated reasoning reflects actual computation:

- **Faithful vs. unfaithful reasoning**: Some chain-of-thought is genuine; some is post-hoc rationalization
- **Planning ahead**: Claude plans future content (e.g., rhymes) before generating, showing longer-horizon thinking than one token at a time
- **Intervention techniques**: Researchers "inject" or "subtract" concepts to test causal relationships

**Relevance**: While this is neural-level research, it highlights that visible thinking blocks may not perfectly represent internal reasoning—a caveat for users analyzing traces.

### Landscape of Thoughts

[Landscape of Thoughts](https://landscape-of-thoughts.github.io/) visualizes reasoning paths using dimensionality reduction:

- Represents intermediate thoughts as feature vectors
- Projects to 2D using t-SNE for visual analysis
- Blue regions = correct paths, red = incorrect
- Shows that wrong paths converge faster than correct ones

**Relevance**: Demonstrates value of visual metaphors for reasoning quality, though more suited for model evaluation than conversation navigation.

### Sources

- [Hippo: Interactive Reasoning (UIST 2025)](https://arxiv.org/html/2506.23678v1)
- [Anthropic: Tracing Thoughts](https://www.anthropic.com/research/tracing-thoughts-language-model)
- [Landscape of Thoughts](https://landscape-of-thoughts.github.io/)
- [Amp Code Homepage](https://ampcode.com/)
- [Amp Owner's Manual](https://ampcode.com/manual)
- [Hamel Husain's Amp Review](https://hamel.dev/notes/coding-agents/amp.html)
- [Ampcode First Impressions - Shantanu Goel](https://shantanugoel.com/2025/06/15/ampcode/)
- [Amp AI Review 2026 - Second Talent](https://www.secondtalent.com/resources/amp-ai-review/)
